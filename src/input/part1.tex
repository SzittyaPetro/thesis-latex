

%Models for object detection

\section{Object detection component implementation}\label{sec:object-detection-component-implementation}

\subsection{Dataset and formats}\label{subsec:dataset-and-formats}

\paragraph{Cityscapes}\label{par:cityscapes}
The Cityscapes dataset is a large-scale dataset~\cite{Cordts2016Cityscapes} used for training and evaluating object detection models.
It contains high-resolution images of urban scenes, with detailed annotations for various objects such as cars,
pedestrians, and traffic signs.
The dataset is widely used in the field of computer vision for tasks such as semantic segmentation and object detection.

I chose the gtFine dataset, consisting precisely labelled segmentation masks.
Based on their work where they published this dataset~\cite{Cordts2016Cityscapes} titled \("\)
Thecityscapes dataset for semantic urban scene understanding\("\) the gtFine dataset consists of 5000 images,
with a resolution of 1024x2048 pixels, and annotations for 30 classes of objects.
It is divided into three subsets: training, validation, and test, with 2975, 500, and 1525 images, respectively.
The validation and training sets contain annotations for 30 classes, while the test set doesn't include any annotations.
The dataset is labelled using pixel-level segmentation masks, which provide detailed information about the
location and shape of objects in the scene.

However for the purpose of this work, the annotations were converted into bounding box format,
which is more suitable and straight forward for this object detection tasks.
Also, the dataset was filtered to include only the classes that are relevant to the project.

\subsection{Data management} \label{subsec:data-management}
The images and annotations are converted into a format that the YOLOv8 model can process, which is
a text file containing the image path and the coordinates of the bounding boxes in pixels for each object in the image.
This process uses a custom script that reads the annotations from the Cityscapes dataset and converts the labels
into labels whose classes are filtered and transformed the relevant classes into the grouping I chose for this project.
The classes were grouped into five categories:
\begin{itemize}
    \item \textbf{small vehicle}\textit{(usually cars, which are for personal use)},
    \item \textbf{large vehicle}\textit{(busses, trucks and other large non personal vehicles)},
    \item \textbf{two wheelers}\textit{(bicycles and motorcycles)},
    \item \textbf{On-rails}\textit{(trains and trams though the smaller Fine dataset didn't include any)}
    \item and \textbf{person} \textit{(pedestrian, and rider)}.
\end{itemize}

The categories were selected on the basis that the model's confusion is more readily managed when objects
that are more similar in appearance are grouped together, thereby facilitating the model's ability to
distinguish between them.

The script also converts the semantic segmentation masks into bounding boxes, trough finding the most
extreme points and of the mask, create a bounding box around them.
This converted output is then saved into a text file, which is in the format used as input for the YOLOv8 model.

The final structure of the dataset with metadata(descriptors and lists) is as follows:
\begin{itemize}
    \item \textbf{Root(gtFine)}: The root directory of the Cityscapes dataset, which contains the images and annotations.
    \begin{itemize}
        \item \textbf{image}: Folder containing the images in the dataset, broken down into training, validation, and test sets
    and further divided into subfolders based on the city where the images were captured.
        \item \textbf{labels}: The class label for each object in the image.
        \item \textbf{train.txt}: The training set, which contains the paths to the training images and their corresponding annotations.
        \item \textbf{val.txt}: The validation set, which contains the paths to the validation images and their corresponding annotations.
        \item \textbf{test.txt}: The test set, which contains the paths to the test images and their corresponding annotations.
    \end{itemize}
    \item \textbf{Descriptors}: The folder containing the class labels and their corresponding indices, as well as
    path set descriptor txt-s.
    It's used by the model to determine the classes, their indices and the paths to the images.
\end{itemize}


\subsection{Model Implementation}\label{subsec:model-implementation}


\subsubsection{Justification behind the selection of YOLOV8}\label{subsubsec:justification-behind-the-selection-of-yolov8}


For my work, I chose the~\textbf{Yolov8m} configuration, which stands for \textbf{Yolo version 8 medium}.
My choice was made on the bases of previous experience with this model architecture and the popularity of its applications, made it so there are an abundance of literature regarding this specific algorithm and architecture.
It is also suitable for numerous, different applications, this aspect was important because of the high variance in sizes and shapes of the traffic objects needed to be detected by the network.

I also examined other networks such as Faster R-CNN and even an older Yolo version, Yolov5.

The Faster R-CNN while being a less sophisticated model it gets outperformed by Yolo on,
a comparison study on these networks \("\)Performance Study of YOLOv5 and Faster R-CNN for
Autonomous Navigation around Non-Cooperative Targets\("\)~\cite{E9843537} it has a table which contains all the necessary information to decide that it is inferior to Yolov8 in almost every aspect


I tested Yolov5 on the same dataset and produced inferior performance KPI(Key Performance Indicator)-s.
I confirmed my findings by a paper that was discussing an application of these models~\cite{Chitraningrum_Banowati_Herdiana_Mulyati_Sakti_Fudholi_Saputra_Farishi_Muchtar_Andria_2024},
Yolov5 exhibits inferior performance relative to Yolov8 on identical input data and with an equivalent architectural configuration.
Additionally it is also faster.
This comparison renders the Yolov5 model obsolete, and therefore it has been excluded from further consideration.

Given its superior performance compared with the two models examined
as well as the extensive availability of related materials, and it's ability to predict bounding boxes and class
probabilities for objects in an image, the YOLOv8 algorithm has been selected for this undertaking.

\paragraph{Chosen model's description and parametrisation}\label{par:model-architecture}

Based on paragraph~\ref{subsubsec:justification-behind-the-selection-of-yolov8} the Yolov8 bounding box
detection model's medium version was picked for this project.
n order to facilitate the requirements of the project, the model was utilised in a pre-trained state,
with the weights being downloaded from the official YOLOv8 repository~\cite{githubGitHubUltralyticsultralytics}.
Subsequently, the model was fine-tuned on the Cityscapes dataset, which was converted into a format compatible
with the model's processing capabilities.
Training was conducted using the Adam optimiser with a learning rate of 0.01 for lr1 and lrf.
I ran the training on the model for 30 epochs with a batch size of 3, to accommodate the limited local GPU memory.
The training process ran on a single PNY NVIDIA Quadro P2000 5GB VCQP2000-PB GPU, with a total training time of 28.056 hours.


\subsubsection{Model evaluation}\label{subsubsec:model-evaluation}
The evaluation of the YOLOv8 model is performed using a set of metrics that measure its performance on the Cityscapes dataset.
These metrics include precision, recall,mAP, and IoU, which are commonly used in object detection tasks.
I used the mAP metric to evaluate the model's performance on the test set, which provides a comprehensive measure of its accuracy.
The mAP is calculated by averaging the precision-recall curves for each class in the dataset, which gives an overall measure of the model's performance.

\[
\text{mAP} = \frac{1}{N} \sum_{i=1}^{N} \text{AP}_i
\]

where \( N \) is the number of classes and \( \text{AP}_i \) is the average precision for class \( i \).

The IoU metric is used to evaluate the model's ability to accurately predict the bounding boxes for objects in the image.
It measures the overlap between the predicted and ground truth bounding boxes, with a higher IoU indicating a more accurate prediction.

\[
\text{IoU} = \frac{\text{Area of Overlap}}{\text{Area of Union}}
\]

The model's performance is evaluated on the test set's all classes averaged:
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Precision & 0.781 \\
\hline
Recall & 0.611 \\
\hline
mAP & 0.41 \\
\hline
IoU & 0.80 \\
\hline
\end{tabular}
\caption{Model performance metrics}
\label{tab:model-performance-metrics}
\end{table}