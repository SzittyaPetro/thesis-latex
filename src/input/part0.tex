
\subsection{Overview of Convolutional Neural Networks}\label{subsec:convolutional-neural-networks}

Convolutional Neural Networks (CNNs) are a class of deep learning models that are mostly used for computer vision tasks
such as image classification, object detection, and segmentation.
Given their ability to learn spatial hierarchies of features and their various types of data representations,
they are perfect for the task of traffic object detection.
Many recent advancements in computer vision have been made possible by CNNs, including the development of models like YOLO, Faster R-CNN, and ResNet.
A dozen of these models have been developed, each with its own unique architecture and capabilities, 
but all of them share the same basic principles discussed by Jiuxiang Gu in his work
\("\)Recent advances in convolutional neural networks\("\) in 2018~\cite{GU2018354}.

The name of the class come from their use of convolutional layers, which apply filters to input data to extract features, outside
convolutional layers, CNNs also contain pooling layers, fully connected layers, and activation functions.

\paragraph{Convolutional layers}\label{par:convolutional-layers}

Convolutional layers are the cornerstones building blocks of convolutional neural networks (CNNs).
Convolution operations are applied to the input data using filters (or kernels) that is processing the input image.
The process enables the model to identify local patterns, including edges, textures, and shapes.
Each convolutional layer generates a set of feature maps, which indicate the presence of diverse features in the input.
During the training phase, the parameters of these filters are learned,
allowing the network to enhance feature extraction for specific tasks.

\paragraph{Pooling layers}\label{par:pooling-layers}

The objective of pooling layers is to reduce the spatial dimensions of feature maps,
thereby decreasing the number of parameters and computations in the network.
Furthermore, this process enhances the model's resilience to minor
translations in the input data by mitigating the effects of potential outliers.
The most prevalent forms of pooling are max pooling and average pooling.
Max pooling selects the maximum value from a specified window, whereas average pooling computes the average.
By down-sampling the feature maps, pooling layers assist in maintaining the most important features while discarding less
critical information from the picture.

\paragraph{Fully connected layers}\label{par:fully-connected-layers}

Fully connected layers, also referred to as dense layers, are typically situated at
the conclusion of convolutional neural network (CNN) architectures.
In these layers, each neuron is connected to every neuron in the preceding layer.
This structure enables the model to integrate information from all features and make final predictions.
Fully connected layers are particularly crucial for classification tasks,
where they compute the output probabilities for each class based on the features extracted by the preceding layers.
Regularisation techniques, such as dropout, are frequently employed in these layers to prevent overfitting.

\paragraph{Sampling layers}\label{par:sampling-layers}

Sampling layers are used to reduce the dimensionality of the data while ensuring the preservation of the input's essential features.
Sampling can entail techniques such as sub-sampling or strided convolutions,
whereby a specific stride is applied to the convolution operation in order to down-sample the feature maps.


\subsection{Activation Functions}\label{subsec:activation-functions}
Activation Functions are used in CNNs(and other Neural networks) to introduce non-linearity into the model.
They assist the model in learning complex patterns and making accurate predictions.
 In light of the work conducted by Siddharth Sharma and Simone Sharma regarding  activation functions~\cite{sharma2017activation}
the most prevalent  activation functions are ReLU, Sigmoid, and Tanh.
Although alternative activation functions, such as BSF and ELU, could be employed, they are not utilised by the examined model (Yolov8).
Consequently, a detailed discussion of these functions will not be provided.
In the followings I will discuss the ReLU, Sigmoid, and Tanh activation functions based on the work of Siddharth and Simone Sharma\cite{sharma2017activation}.

\paragraph{ReLU}\label{par:relu}
The Rectified Linear Unit (ReLU) is one of the most widely used activation functions in convolutional neural networks (CNNs).
It is defined as \( f(x) = \max(0, x) \) by~\cite{sharma2017activation}.
This function introduces non-linearity while maintaining a simple and efficient computation.
ReLU helps mitigate the vanishing gradient problem, allowing models to learn faster and perform better.
However, it can suffer from the \("\)dying ReLU\("\) problem, where neurons become inactive and only output zeros,
particularly during the training of deep networks.

\paragraph{Sigmoid}\label{par:sigmoid}
The Sigmoid function maps input values to a range between 0 and 1,
making it useful for binary classification problems.
It is defined as \( f(x) = \frac{1}{1 + e^{-x}} \) by~\cite{sharma2017activation}.
While the Sigmoid function provides smooth gradients,
it is prone to the vanishing gradient problem,
especially for large positive or negative input values.
This can slow down the training of deep networks,
which is why it is often replaced by other activation
functions in hidden layers,
though it still finds use in the output layer for binary classification tasks.

\paragraph{Tanh}\label{par:tanh}
The Hyperbolic Tangent (Tanh) function is another activation function that maps input values to a range between -1 and 1.
It is defined as  \( f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \) by~\cite{sharma2017activation}.
Tanh is zero-centered and centrically symmetrical, which helps in centering the data and can lead to faster convergence during training.
However, it still suffers from the vanishing gradient problem for large input values, though to a lesser extent than the Sigmoid function.